{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "limited-baltimore",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-30T05:26:51.262396Z",
     "iopub.status.busy": "2021-07-30T05:26:51.262052Z",
     "iopub.status.idle": "2021-07-30T05:26:52.830081Z",
     "shell.execute_reply": "2021-07-30T05:26:52.828619Z",
     "shell.execute_reply.started": "2021-07-30T05:26:51.262318Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs/LibsvmDataset.html\n"
     ]
    }
   ],
   "source": [
    "!pdoc --html --output-dir docs LibsvmDataset.py --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "racial-active",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-30T05:38:33.321009Z",
     "iopub.status.busy": "2021-07-30T05:38:33.320771Z",
     "iopub.status.idle": "2021-07-30T05:38:33.327675Z",
     "shell.execute_reply": "2021-07-30T05:38:33.326601Z",
     "shell.execute_reply.started": "2021-07-30T05:38:33.320984Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7\n",
      "3.53.1\n",
      "0.22.1\n",
      "1.19.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request \n",
    "import progressbar\n",
    "import sklearn.datasets\n",
    "import numpy\n",
    "print(urllib.request.__version__)\n",
    "print(progressbar.__version__)\n",
    "print(sklearn.__version__)\n",
    "print(numpy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "elder-deposit",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-30T05:26:53.290637Z",
     "iopub.status.busy": "2021-07-30T05:26:53.290379Z",
     "iopub.status.idle": "2021-07-30T05:26:54.000653Z",
     "shell.execute_reply": "2021-07-30T05:26:53.999726Z",
     "shell.execute_reply.started": "2021-07-30T05:26:53.290606Z"
    }
   },
   "outputs": [],
   "source": [
    "from LibsvmDataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "joint-protocol",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-30T05:28:41.277036Z",
     "iopub.status.busy": "2021-07-30T05:28:41.276743Z",
     "iopub.status.idle": "2021-07-30T05:28:45.575580Z",
     "shell.execute_reply": "2021-07-30T05:28:45.574629Z",
     "shell.execute_reply.started": "2021-07-30T05:28:41.277005Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case 0: task is not available\n",
      "======================\n",
      "You choose to use the task+dataset option.\n",
      "\u001b[93mWarning:Your input taks is [multi-class], which currently is not supported.\n",
      "However, you can provide an url pointing to the desired dataset to download it.\u001b[0m\n",
      "\u001b[93mFail to generate download url, please check your inputs.\u001b[0m\n",
      "Case 1: dataset is not avaiable\n",
      "======================\n",
      "You choose to use the task+dataset option.\n",
      "\u001b[91mError occurs!\n",
      "  1.Either the input dataset:[a0a] is not intended for the task:[binary].\n",
      "  2.Or the input dataset:[a0a] is not in the built-in database.\n",
      "If you are sure the latter case happens, you can provide an url pointing to the desired dataset.\u001b[0m\n",
      "\u001b[93mFail to generate download url, please check your inputs.\u001b[0m\n",
      "Case 2: dataset doesn't match the task\n",
      "======================\n",
      "You choose to use the task+dataset option.\n",
      "\u001b[91mError occurs!\n",
      "  1.Either the input dataset:[a1a] is not intended for the task:[regression].\n",
      "  2.Or the input dataset:[a1a] is not in the built-in database.\n",
      "If you are sure the latter case happens, you can provide an url pointing to the desired dataset.\u001b[0m\n",
      "\u001b[93mFail to generate download url, please check your inputs.\u001b[0m\n",
      "Case 3: incorrect inputs combination\n",
      "======================\n",
      "You choose to use the url option.\n",
      "\u001b[91mThe input url www.google.com is wrong.\u001b[0m\n",
      "\u001b[93mFail to generate download url, please check your inputs.\u001b[0m\n",
      "Case 4: correct task+dataset\n",
      "======================\n",
      "You choose to use the task+dataset option.\n",
      "Parsed task: [binary] | Parsed dataset: [a1a]\n",
      "Parsed download_url:[https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/a1a]\n",
      "\u001b[93mThe dataset [a1a] already exists in [./raw/binary]!\u001b[0m\n",
      "\u001b[93mThe cleaned dataset [a1a] already exists in [./clean/binary]!\u001b[0m\n",
      "Case 5: get dataset for task: binary| dataset: a1a.t\n",
      "======================\n",
      "You choose to use the url option.\n",
      "Parsed task: [binary] | Parsed dataset: [a1a.t]\n",
      "Parsed download_url:[https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/a1a.t]\n",
      "\u001b[93mThe dataset [a1a.t] already exists in [./raw/binary]!\u001b[0m\n",
      "\u001b[93mThe cleaned dataset [a1a.t] already exists in [./clean/binary]!\u001b[0m\n",
      "Case 6: get dataset for task: multi-class\n",
      "======================\n",
      "You choose to use the url option.\n",
      "Parsed task: [multiclass] | Parsed dataset: [aloi.bz2]\n",
      "Parsed download_url:[https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/aloi.bz2]\n",
      "\u001b[93mThe dataset [aloi.bz2] already exists in [./raw/multiclass]!\u001b[0m\n",
      "\u001b[93mThe clean rule for dataset:aloi.bz2 with task:multiclass is not defined. Hence, no cleaning is performed.\u001b[0m\n",
      "Case 7: correct task+dataset with force_download without force_clean\n",
      "======================\n",
      "You choose to use the task+dataset option.\n",
      "Parsed task: [binary] | Parsed dataset: [a1a]\n",
      "Parsed download_url:[https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/a1a]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (114818 of 114818) |################| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mdataset [a1a] is downloaded at [./raw/binary].\u001b[0m\n",
      "\u001b[93mThe cleaned dataset [a1a] already exists in [./clean/binary]!\u001b[0m\n",
      "Case 8: correct task+dataset with force_download with force_clean\n",
      "======================\n",
      "You choose to use the task+dataset option.\n",
      "Parsed task: [binary] | Parsed dataset: [a1a]\n",
      "Parsed download_url:[https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/a1a]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (114818 of 114818) |################| Elapsed Time: 0:00:01 Time:  0:00:01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mdataset [a1a] is downloaded at [./raw/binary].\u001b[0m\n",
      "Original y-label range: { -1.0, 1.0 } -> New y-label range: { -1.0, 1.0 }\n",
      "Perform normalization:feat-11\n",
      "\u001b[92mSuccess: File saved at ./clean/binary/a1a!\u001b[0m\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n"
     ]
    }
   ],
   "source": [
    "# unit tests\n",
    "libclass = LibsvmDataset()\n",
    "# libclass.getAvailable()\n",
    "print(\"Case 0: task is not available\\n======================\")\n",
    "libclass.getAndClean(\"multi-class\", \"aloi.bz2\")\n",
    "print(\"Case 1: dataset is not avaiable\\n======================\")\n",
    "libclass.getAndClean(\"binary\", \"a0a\")\n",
    "print(\"Case 2: dataset doesn't match the task\\n======================\")\n",
    "libclass.getAndClean(\"regression\", \"a1a\")\n",
    "print(\"Case 3: incorrect inputs combination\\n======================\")\n",
    "libclass.getAndClean(task=\"binary\", download_url=\"www.google.com\")\n",
    "print(\"Case 4: correct task+dataset\\n======================\")\n",
    "libclass.getAndClean(\"binary\", \"a1a\")\n",
    "print(\"Case 5: get dataset for task: binary| dataset: a1a.t\\n======================\")\n",
    "libclass.getAndClean(download_url=\"https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/a1a.t\")\n",
    "print(\"Case 6: get dataset for task: multi-class\\n======================\")\n",
    "libclass.getAndClean(download_url=\"https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/aloi.bz2\")\n",
    "print(\"Case 7: correct task+dataset with force_download without force_clean\\n======================\")\n",
    "libclass.getAndClean(task=\"binary\", dataset=\"a1a\", force_download=True, force_clean=False)\n",
    "print(\"Case 8: correct task+dataset with force_download with force_clean\\n======================\")\n",
    "libclass.getAndClean(task=\"binary\", dataset=\"a1a\", force_download=True, force_clean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "nuclear-algebra",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-30T05:25:37.088666Z",
     "iopub.status.busy": "2021-07-30T05:25:37.088446Z",
     "iopub.status.idle": "2021-07-30T05:25:37.135374Z",
     "shell.execute_reply": "2021-07-30T05:25:37.134230Z",
     "shell.execute_reply.started": "2021-07-30T05:25:37.088642Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "import progressbar\n",
    "from sklearn.datasets import load_svmlight_file, dump_svmlight_file\n",
    "import numpy as np\n",
    "# import subprocess\n",
    "# import shlex\n",
    "\n",
    "class bcolors:\n",
    "    \"\"\"\n",
    "        Define colors for terminal output texts.\n",
    "    \"\"\"\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "\n",
    "class LibsvmDataset:\n",
    "    def __init__(self, download_dir=\"./raw\", \n",
    "                 cleand_dir=\"./clean\"):\n",
    "        \"\"\"\n",
    "        Initialize the  LibsvmDataset class.\n",
    "        \n",
    "        Args:\n",
    "            download_dir: A string specifies the place to store the downloaded raw dataset.\n",
    "            cleand_dir: A string specifies the place to store the cleaned dataset.\n",
    "        \"\"\"\n",
    "        self.download_dir = download_dir\n",
    "        self.cleand_dir = cleand_dir\n",
    "        for directory in [self.download_dir, self.cleand_dir]:\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "        self.url_regression = \"https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/regression\"\n",
    "        self.url_binary = \"https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary\"\n",
    "        self.data_binary = [\n",
    "            'a1a', 'a2a', 'a3a', 'a4a', 'a5a', 'a6a', 'a7a', 'a8a', 'a9a',\n",
    "            'a1a.t', 'a2a.t', 'a3a.t', 'a4a.t', 'a5a.t', 'a6a.t', 'a7a.t', 'a8a.t', 'a9a.t', \n",
    "            'australian',\n",
    "            'breast-cancer',\n",
    "            'cod-rna', 'cod-rna.t', 'cod-rna.r', \n",
    "            'colon-cancer.bz2',\n",
    "            'covtype.libsvm.binary.bz2',\n",
    "            'diabetes',\n",
    "            'duke.bz2',\n",
    "            'fourclass',\n",
    "            'german.numer',\n",
    "            'gisette_scale.bz2', 'gisette_scale.t.bz2',\n",
    "            'heart',\n",
    "            'ijcnn1.bz2',\n",
    "            'ionosphere_scale',\n",
    "            'leu.bz2', 'leu.bz2.t',\n",
    "            'liver-disorders', 'liver-disorders.t',\n",
    "            'madelon', 'madelon.t',\n",
    "            'mushrooms',\n",
    "            'news20.binary.bz2',\n",
    "            'phishing',\n",
    "            'rcv1_train.binary.bz2','rcv1_test.binary.bz2',\n",
    "            'real-sim.bz2', 'skin_nonskin',\n",
    "            'splice', 'splice.t',\n",
    "            'sonar_scale',\n",
    "            'svmguide1', 'svmguide1.t', 'svmguide3', 'svmguide3.t', \n",
    "            'w1a', 'w2a', 'w3a', 'w4a', 'w5a', 'w6a', 'w7a', 'w8a',\n",
    "            'w1a.t', 'w2a.t', 'w3a.t', 'w4a.t', 'w5a.t', 'w6a.t', 'w7a.t', 'w8a.t'\n",
    "             #'epsilon_normalized.bz2', 'epsilon_normalized.t.bz2'\n",
    "             #'HIGGS.bz2',\n",
    "        ]        \n",
    "        self.data_regression = [\n",
    "            'abalone',\n",
    "            'bodyfat',\n",
    "            'cadata',\n",
    "            'cpusmall',\n",
    "            'log1p.E2006.train.bz2', 'log1p.E2006.test.bz2 '\n",
    "            'E2006.train.bz2', 'E2006.test.bz2',\n",
    "            'eunite2001', 'eunite2001.t', 'eunite2001.m',\n",
    "            'housing',\n",
    "            'mg',\n",
    "            'mpg',\n",
    "            'pyrim',\n",
    "            'space_ga',\n",
    "            'triazines',\n",
    "            'YearPredictionMSD.bz2', 'YearPredictionMSD.t.bz2'\n",
    "        ]       \n",
    "        self.task_dict = {\"binary\":{\"url\":self.url_binary, \n",
    "                                    \"dataset\":self.data_binary}, \n",
    "                          \"regression\":{\"url\":self.url_regression, \n",
    "                                        \"dataset\":self.data_regression}}  \n",
    "        # for printing\n",
    "        self.pbar = None\n",
    "        \n",
    "    def _show_progress(self, block_num, block_size, total_size):\n",
    "        \"\"\"\n",
    "            private function. Show the progress of urlretrieve for downloading data.\n",
    "        \"\"\"\n",
    "        if self.pbar is None:\n",
    "            self.pbar = progressbar.ProgressBar(maxval=total_size)\n",
    "            self.pbar.start()\n",
    "\n",
    "        downloaded = block_num * block_size\n",
    "        if downloaded < total_size:\n",
    "            self.pbar.update(downloaded)\n",
    "        else:\n",
    "            self.pbar.finish()\n",
    "            self.pbar = None\n",
    "    \n",
    "    def _parseInputs(self, task=None, dataset=None, download_url=None):\n",
    "        if task is not None and dataset is not None:\n",
    "            print(\"You choose to use the task+dataset option.\")\n",
    "            try:\n",
    "                work_dict = self.task_dict[task]\n",
    "            except KeyError:\n",
    "                print(f\"{bcolors.WARNING}Warning:Your input taks is [{task}], which currently is not supported.\\n\"\\\n",
    "                      f\"However, you can provide an url pointing to the desired dataset to download it.{bcolors.ENDC}\")\n",
    "                return\n",
    "            is_available = dataset in work_dict[\"dataset\"]\n",
    "            if not is_available:\n",
    "                print(f\"{bcolors.FAIL}Error occurs!\\n\"\\\n",
    "                     f\"  1.Either the input dataset:[{dataset}] is not intended for the task:[{task}].\\n\"\\\n",
    "                     f\"  2.Or the input dataset:[{dataset}] is not in the built-in database.\\n\"\\\n",
    "                     f\"If you are sure the latter case happens, you can provide an url pointing to the desired dataset.{bcolors.ENDC}\"\n",
    "                     )\n",
    "                return\n",
    "            self.download_url = work_dict[\"url\"] + \"/\" + dataset\n",
    "            self.task = task\n",
    "            self.dataset = dataset\n",
    "        elif download_url:\n",
    "            print(\"You choose to use the url option.\")\n",
    "            try:\n",
    "                task, dataset = download_url.split(\"/\")[-2], download_url.split(\"/\")[-1]\n",
    "                self.download_url = download_url\n",
    "                self.task = task\n",
    "                self.dataset = dataset\n",
    "            except IndexError:\n",
    "                self.download_url = None\n",
    "                print(f\"{bcolors.FAIL}The input url {download_url} is wrong.{bcolors.ENDC}\")\n",
    "        else:\n",
    "            raise ValueError(f\"{bcolors.FAIL}Code has bugs.{bcolors.ENDC}\")\n",
    "        if self.download_url:\n",
    "            print(f\"Parsed task: [{self.task}] | Parsed dataset: [{self.dataset}]\\nParsed download_url:[{self.download_url}]\")\n",
    "    def _getData(self, force_download):\n",
    "        \"\"\"\n",
    "            Use urllib.request::urlretrieve to download the self.dataset and save to \n",
    "            self.download_dir/self.task based on the self.download_url.\n",
    "            If the dataset already exists, then it will skip. However, you can force download\n",
    "            by setting force_download=True.\n",
    "        \"\"\"\n",
    "        if self.download_url is not None:\n",
    "            # check whether the dataset is being downloaded or not\n",
    "            directory = f\"{self.download_dir}/{self.task}\"\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            is_downloaded = os.path.exists(f\"{directory}/{self.dataset}\")\n",
    "            if not is_downloaded or force_download:\n",
    "                urlretrieve(self.download_url, f'{directory}/{self.dataset}', self._show_progress)\n",
    "                #print(\"Start downloading... It may take a while\")\n",
    "                #subprocess.run(['wget', '-i', self.download_url, '-P', self.download_dir, \n",
    "                #                '-O', f'{self.download_dir}/{self.dataset}'])\n",
    "                if os.path.exists(f\"{directory}/{self.dataset}\"):\n",
    "                    print(f\"{bcolors.OKGREEN}dataset [{self.dataset}] is downloaded at [{directory}].{bcolors.ENDC}\")\n",
    "            else:\n",
    "                print(f\"{bcolors.WARNING}The dataset [{self.dataset}] already exists in [{directory}]!{bcolors.ENDC}\")\n",
    "    def _cleanData(self, normalization, binary_label, force_clean):\n",
    "        \"\"\"\n",
    "            Load the raw dataset from self.download_dir.\n",
    "            If normalization is set, it will perform appropriate normalization.\n",
    "            See docstring of the getAndClean to find valid values.\n",
    "            \n",
    "            If the cleaned dataset already exists, then it will skip. \n",
    "            However, you can force cleaning the dataset by setting force_clean=True.\n",
    "        \"\"\"\n",
    "        directory = f\"{self.cleand_dir}/{self.task}\"\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        is_cleaned = os.path.exists(f\"{directory}/{self.dataset}\")\n",
    "        if not is_cleaned or force_clean:\n",
    "            data = load_svmlight_file(f\"{self.download_dir}/{self.task}/{self.dataset}\")\n",
    "            X, y= data[0], data[1]\n",
    "            n, p = X.shape\n",
    "            # check label for the binary task\n",
    "            if self.task == 'binary':\n",
    "                y1old, y2old = np.unique(y)\n",
    "                if binary_label is not None:\n",
    "                    if  binary_label=='{-1,1}':\n",
    "                        y1new, y2new = -1.0, 1.0\n",
    "                    elif binary_label=='{0,1}':\n",
    "                        y1new, y2new = 0.0, 1.0\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unrecognized binary_level: {binary_label}\")\n",
    "                    y[y==y1old] = y1new\n",
    "                    y[y==y2old] = y2new\n",
    "                    print(f\"Original y-label range: {{ {y1old}, {y2old} }} -> New y-label range: {{ {np.unique(y)[0]}, {np.unique(y)[1]} }}\")\n",
    "                else:\n",
    "                    raise ValueError(f\"{bcolors.FAIL}You should set the desired binary_level.\\n For example, binary_label='[-1,1]'.{bcolors.ENDC}\")\n",
    "            # check feature range\n",
    "            if normalization is not None:\n",
    "                print(f\"Perform normalization:{normalization}\")\n",
    "                if normalization == 'feat-11':\n",
    "                    for i in range(p):\n",
    "                        temp = X[:,i]\n",
    "                        if np.max(temp) > 1.0 or np.min(temp) < -1.0:\n",
    "                            X[:,i] /= np.max(np.abs(temp))\n",
    "                            if verbose:\n",
    "                                print(f\"  col:{i}: max:{np.max(temp):3.3e} | min:{np.min(temp):3.3e}\\n\"\\\n",
    "                                       \"  Apply feature-wise [-1,1] scaling...\")\n",
    "                elif normalization == 'feat01':\n",
    "                    for i in range(p):\n",
    "                        temp = X[:,i]\n",
    "                        xmax, xmin = np.max(temp), np.min(temp)\n",
    "                        if xmax > 1.0 or  xmin < 0.0:\n",
    "                            X[:,i] = (X[:,i] - xmin) / (xmax - xmin)\n",
    "                            if verbose:\n",
    "                                print(f\"  col:{i}: max:{np.max(temp):3.3e} | min:{np.min(temp):3.3e}\\n\"\\\n",
    "                                       \"  Apply feature-wise [0,1] scaling...\")\n",
    "                else:\n",
    "                    raise ValueError(f\"{bcolors.FAIL}Unrecognized normalization: {normalization}{bcolors.ENDC}\")\n",
    "            dump_svmlight_file(X, y, f\"{directory}/{self.dataset}\")\n",
    "            if os.path.exists(f\"{directory}/{self.dataset}\"):\n",
    "                print(f\"{bcolors.OKGREEN}Success: File saved at {directory}/{self.dataset}!{bcolors.ENDC}\")\n",
    "                print(\"-*\"*30)\n",
    "        else:\n",
    "             print(f\"{bcolors.WARNING}The cleaned dataset [{self.dataset}] already exists in [{directory}]!{bcolors.ENDC}\")\n",
    "    def getAvailable(self):\n",
    "            \"\"\"Show supported tasks and for each supported task show avaiable datasets.\n",
    "            Typical usage example:\n",
    "            \n",
    "                   libsvm = LibsvmDataset()\n",
    "                   libsvm.getAvailable()\n",
    "            \"\"\"\n",
    "            print(\"Current supported tasks are:\")\n",
    "            for k in self.task_dict.keys():\n",
    "                print(f\" ['{k}']\", end=\"\")\n",
    "            print(\"\\n=====================================\")\n",
    "            for k in self.task_dict.keys():\n",
    "                print(f\"For task:['{k}'], available datasets are:\")\n",
    "                print(\"----------------------------------------------------\")\n",
    "                dataset_lst = []\n",
    "                for i,d in enumerate(self.task_dict[k][\"dataset\"]):\n",
    "                    print(f\" '{d}'\", end=\",\")\n",
    "                    if (i +1) % 5 == 0:\n",
    "                        print(\"\\n\")\n",
    "            print(\"\\n\")\n",
    "    def getAndClean(self, task=None, dataset=None, download_url=None, \n",
    "                    binary_lable='{-1,1}', normalization='feat-11',\n",
    "                    force_download=False, force_clean=False, clean_verbose=True\n",
    "                   ):\n",
    "        \"\"\"Download and clean the dataset.\n",
    "        Typical usage example:\n",
    "\n",
    "              libsvm = LibsvmDataset()\n",
    "              libsvm.getAndClean(task=\"binary\", dataset=\"a1a\", binary_lable='{-1,1}', normalization='feat-11')\n",
    "              libsvm.getAndClean(task=\"regression\", dataset=\"abalone\", normalization='feat-11')\n",
    "              libsvm.getAndClean(url='https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/avazu-app.bz2',\n",
    "                                 binary_lable='{-1,1}', normalization='feat-11')              \n",
    "        Args:\n",
    "\n",
    "            task: A string specifies the task want to perform. Currently supported {'binary', 'regression'}.\n",
    "            dataset: A string specifies the dataset you want to download. Use `getAvailable` method to show all\n",
    "                     currently avaiable datasets for any given task.\n",
    "            download_url: If the desired dataset is not provided for a given task, one can directly provide a url \n",
    "                          link to the desired data set. For example, one wants to download the avazu dataset.\n",
    "                          One can visit https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html#avazu.\n",
    "                          If you want to download the \"avazu-app.bz2\" instance, you can right-click its name and \n",
    "                          select \"copy link\", then you should get a plain text as\n",
    "                              <https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/avazu-app.bz2>\n",
    "                          Then just provides it as string.\n",
    "            binary_label: If you want to perform binay classification, one can set labels to {-1,1} by providing\n",
    "                          '{-1,1}'; or set labels to {0,1} by providing '{0,1}'. Default to '{-1,1}'.\n",
    "            normalization: Perform feature-wise normalization. Currently supported options:\n",
    "                             'feat-11': feature-wise scaling to range [-1,1]\n",
    "                             'feat01': feature-wise scaling to range [0,1]\n",
    "                           Default to '{-1,1}'.\n",
    "            force_download: If set to True, then download the dataset even if it already exists. Default to False.\n",
    "            force_clean:    If set to True, then clean the dataset even if it already exists in clean folder. Default to False.\n",
    "            clean_verbose:  If set to True, will print out which feature being normalized. Default to False.\n",
    "        \"\"\"\n",
    "        # reset\n",
    "        self.download_url = None\n",
    "        self.task = None\n",
    "        self.dataset = None \n",
    "        self._parseInputs(task, dataset, download_url)\n",
    "        if self.download_url is not None:\n",
    "            self._getData(force_download)\n",
    "            if self.task in [\"binary\", \"regression\"]:\n",
    "                self._cleanData(normalization, binary_lable, force_clean)\n",
    "            else:\n",
    "                print(f\"{bcolors.WARNING}The clean rule for dataset:{self.dataset} with task:{self.task} is not defined. Hence, no cleaning is performed.{bcolors.ENDC}\")\n",
    "        else:\n",
    "            print(f\"{bcolors.WARNING}Fail to generate download url, please check your inputs.{bcolors.ENDC}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "collected-platinum",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-30T05:25:37.764425Z",
     "iopub.status.busy": "2021-07-30T05:25:37.764199Z",
     "iopub.status.idle": "2021-07-30T05:25:37.769996Z",
     "shell.execute_reply": "2021-07-30T05:25:37.768890Z",
     "shell.execute_reply.started": "2021-07-30T05:25:37.764399Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You choose to use the url option.\n",
      "\u001b[91mThe input url www.google.com is wrong.\u001b[0m\n",
      "\u001b[93mFail to generate download url, please check your inputs.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "libclass = LibsvmDataset()\n",
    "libclass.getAndClean(task=\"binary\", download_url=\"www.google.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "stock-chemical",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-30T02:51:01.666692Z",
     "iopub.status.busy": "2021-07-30T02:51:01.666391Z",
     "iopub.status.idle": "2021-07-30T02:51:01.817785Z",
     "shell.execute_reply": "2021-07-30T02:51:01.816031Z",
     "shell.execute_reply.started": "2021-07-30T02:51:01.666654Z"
    }
   },
   "outputs": [],
   "source": [
    "rm -rf ./raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "completed-parker",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-30T02:19:55.742358Z",
     "iopub.status.busy": "2021-07-30T02:19:55.742086Z",
     "iopub.status.idle": "2021-07-30T02:19:57.645494Z",
     "shell.execute_reply": "2021-07-30T02:19:57.644265Z",
     "shell.execute_reply.started": "2021-07-30T02:19:55.742328Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You choose to use the task+dataset option.\n",
      "Parsed task: [binary] | Parsed dataset: [a1a]\n",
      "Parsed download_url:[https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/a1a]\n",
      "dataset [a1a] is downloaded at [./raw].\n"
     ]
    }
   ],
   "source": [
    "# libclass = LibsvmDataset()\n",
    "# libclass.getAndClean(\"binary\", \"a1a\", force_download=True)\n",
    "# libclass._parseInputs(\"binary\", \"a1a\", None)\n",
    "# subprocess.run(['wget', '-i', libclass.download_url, '-P', libclass.download_dir], capture_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cosmetic-smile",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-30T02:36:05.185005Z",
     "iopub.status.busy": "2021-07-30T02:36:05.184774Z",
     "iopub.status.idle": "2021-07-30T02:36:10.255207Z",
     "shell.execute_reply": "2021-07-30T02:36:10.254321Z",
     "shell.execute_reply.started": "2021-07-30T02:36:05.184979Z"
    }
   },
   "outputs": [],
   "source": [
    "# import requests\n",
    "# r = requests.get(libclass.download_url, allow_redirects=True)\n",
    "# with open(f'{libclass.download_dir}/{libclass.dataset}', 'wb') as f:\n",
    "#     f.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-scheduling",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
