<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>LibsvmDataset API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>LibsvmDataset</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import os
from urllib.request import urlretrieve
import progressbar
from sklearn.datasets import load_svmlight_file, dump_svmlight_file
import numpy as np
# import subprocess
# import shlex

class _bcolors:
    &#34;&#34;&#34;
        Define colors for terminal output texts.
    &#34;&#34;&#34;
    HEADER = &#39;\033[95m&#39;
    OKBLUE = &#39;\033[94m&#39;
    OKCYAN = &#39;\033[96m&#39;
    OKGREEN = &#39;\033[92m&#39;
    WARNING = &#39;\033[93m&#39;
    FAIL = &#39;\033[91m&#39;
    ENDC = &#39;\033[0m&#39;
    BOLD = &#39;\033[1m&#39;
    UNDERLINE = &#39;\033[4m&#39;

class LibsvmDataset:
    def __init__(self, download_dir=&#34;./raw&#34;, 
                 cleand_dir=&#34;./clean&#34;):
        &#34;&#34;&#34;
        Initialize the  LibsvmDataset class.
        
        Args:
            download_dir: A string specifies the place to store the downloaded raw dataset.
            cleand_dir: A string specifies the place to store the cleaned dataset.
        &#34;&#34;&#34;
        self.download_dir = download_dir
        self.cleand_dir = cleand_dir
        for directory in [self.download_dir, self.cleand_dir]:
            if not os.path.exists(directory):
                os.makedirs(directory)
        self.url_regression = &#34;https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/regression&#34;
        self.url_binary = &#34;https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary&#34;
        self.data_binary = [
            &#39;a1a&#39;, &#39;a2a&#39;, &#39;a3a&#39;, &#39;a4a&#39;, &#39;a5a&#39;, &#39;a6a&#39;, &#39;a7a&#39;, &#39;a8a&#39;, &#39;a9a&#39;,
            &#39;a1a.t&#39;, &#39;a2a.t&#39;, &#39;a3a.t&#39;, &#39;a4a.t&#39;, &#39;a5a.t&#39;, &#39;a6a.t&#39;, &#39;a7a.t&#39;, &#39;a8a.t&#39;, &#39;a9a.t&#39;, 
            &#39;australian&#39;,
            &#39;breast-cancer&#39;,
            &#39;cod-rna&#39;, &#39;cod-rna.t&#39;, &#39;cod-rna.r&#39;, 
            &#39;colon-cancer.bz2&#39;,
            &#39;covtype.libsvm.binary.bz2&#39;,
            &#39;diabetes&#39;,
            &#39;duke.bz2&#39;,
            &#39;fourclass&#39;,
            &#39;german.numer&#39;,
            &#39;gisette_scale.bz2&#39;, &#39;gisette_scale.t.bz2&#39;,
            &#39;heart&#39;,
            &#39;ijcnn1.bz2&#39;,
            &#39;ionosphere_scale&#39;,
            &#39;leu.bz2&#39;, &#39;leu.bz2.t&#39;,
            &#39;liver-disorders&#39;, &#39;liver-disorders.t&#39;,
            &#39;madelon&#39;, &#39;madelon.t&#39;,
            &#39;mushrooms&#39;,
            &#39;news20.binary.bz2&#39;,
            &#39;phishing&#39;,
            &#39;rcv1_train.binary.bz2&#39;,&#39;rcv1_test.binary.bz2&#39;,
            &#39;real-sim.bz2&#39;, &#39;skin_nonskin&#39;,
            &#39;splice&#39;, &#39;splice.t&#39;,
            &#39;sonar_scale&#39;,
            &#39;svmguide1&#39;, &#39;svmguide1.t&#39;, &#39;svmguide3&#39;, &#39;svmguide3.t&#39;, 
            &#39;w1a&#39;, &#39;w2a&#39;, &#39;w3a&#39;, &#39;w4a&#39;, &#39;w5a&#39;, &#39;w6a&#39;, &#39;w7a&#39;, &#39;w8a&#39;,
            &#39;w1a.t&#39;, &#39;w2a.t&#39;, &#39;w3a.t&#39;, &#39;w4a.t&#39;, &#39;w5a.t&#39;, &#39;w6a.t&#39;, &#39;w7a.t&#39;, &#39;w8a.t&#39;
             #&#39;epsilon_normalized.bz2&#39;, &#39;epsilon_normalized.t.bz2&#39;
             #&#39;HIGGS.bz2&#39;,
        ]        
        self.data_regression = [
            &#39;abalone&#39;,
            &#39;bodyfat&#39;,
            &#39;cadata&#39;,
            &#39;cpusmall&#39;,
            &#39;log1p.E2006.train.bz2&#39;, &#39;log1p.E2006.test.bz2 &#39;
            &#39;E2006.train.bz2&#39;, &#39;E2006.test.bz2&#39;,
            &#39;eunite2001&#39;, &#39;eunite2001.t&#39;, &#39;eunite2001.m&#39;,
            &#39;housing&#39;,
            &#39;mg&#39;,
            &#39;mpg&#39;,
            &#39;pyrim&#39;,
            &#39;space_ga&#39;,
            &#39;triazines&#39;,
            &#39;YearPredictionMSD.bz2&#39;, &#39;YearPredictionMSD.t.bz2&#39;
        ]       
        self.task_dict = {&#34;binary&#34;:{&#34;url&#34;:self.url_binary, 
                                    &#34;dataset&#34;:self.data_binary}, 
                          &#34;regression&#34;:{&#34;url&#34;:self.url_regression, 
                                        &#34;dataset&#34;:self.data_regression}}  
        # for printing
        self.pbar = None
        
    def _show_progress(self, block_num, block_size, total_size):
        &#34;&#34;&#34;
            private function. Show the progress of urlretrieve for downloading data.
        &#34;&#34;&#34;
        if self.pbar is None:
            self.pbar = progressbar.ProgressBar(maxval=total_size)
            self.pbar.start()

        downloaded = block_num * block_size
        if downloaded &lt; total_size:
            self.pbar.update(downloaded)
        else:
            self.pbar.finish()
            self.pbar = None
    
    def _parseInputs(self, task=None, dataset=None, download_url=None):
        if task is not None and dataset is not None:
            print(&#34;You choose to use the task+dataset option.&#34;)
            try:
                work_dict = self.task_dict[task]
            except KeyError:
                print(f&#34;{_bcolors.WARNING}Warning:Your input taks is [{task}], which currently is not supported.\n&#34;\
                      f&#34;However, you can provide an url pointing to the desired dataset to download it.{_bcolors.ENDC}&#34;)
                return
            is_available = dataset in work_dict[&#34;dataset&#34;]
            if not is_available:
                print(f&#34;{_bcolors.FAIL}Error occurs!\n&#34;\
                     f&#34;  1.Either the input dataset:[{dataset}] is not intended for the task:[{task}].\n&#34;\
                     f&#34;  2.Or the input dataset:[{dataset}] is not in the built-in database.\n&#34;\
                     f&#34;If you are sure the latter case happens, you can provide an url pointing to the desired dataset.{_bcolors.ENDC}&#34;
                     )
                return
            self.download_url = work_dict[&#34;url&#34;] + &#34;/&#34; + dataset
            self.task = task
            self.dataset = dataset
        elif download_url:
            print(&#34;You choose to use the url option.&#34;)
            try:
                task, dataset = download_url.split(&#34;/&#34;)[-2], download_url.split(&#34;/&#34;)[-1]
                self.download_url = download_url
                self.task = task
                self.dataset = dataset
            except IndexError:
                self.download_url = None
                print(f&#34;{_bcolors.FAIL}The input url {download_url} is wrong.{_bcolors.ENDC}&#34;)
        else:
            raise ValueError(f&#34;{_bcolors.FAIL}Code has bugs.{_bcolors.ENDC}&#34;)
        if self.download_url:
            print(f&#34;Parsed task: [{self.task}] | Parsed dataset: [{self.dataset}]\nParsed download_url:[{self.download_url}]&#34;)
    def _getData(self, force_download):
        &#34;&#34;&#34;
            Use urllib.request::urlretrieve to download the self.dataset and save to 
            self.download_dir/self.task based on the self.download_url.
            If the dataset already exists, then it will skip. However, you can force download
            by setting force_download=True.
        &#34;&#34;&#34;
        if self.download_url is not None:
            # check whether the dataset is being downloaded or not
            directory = f&#34;{self.download_dir}/{self.task}&#34;
            if not os.path.exists(directory):
                os.makedirs(directory)
            is_downloaded = os.path.exists(f&#34;{directory}/{self.dataset}&#34;)
            if not is_downloaded or force_download:
                urlretrieve(self.download_url, f&#39;{directory}/{self.dataset}&#39;, self._show_progress)
                #print(&#34;Start downloading... It may take a while&#34;)
                #subprocess.run([&#39;wget&#39;, &#39;-i&#39;, self.download_url, &#39;-P&#39;, self.download_dir, 
                #                &#39;-O&#39;, f&#39;{self.download_dir}/{self.dataset}&#39;])
                if os.path.exists(f&#34;{directory}/{self.dataset}&#34;):
                    print(f&#34;{_bcolors.OKGREEN}dataset [{self.dataset}] is downloaded at [{directory}].{_bcolors.ENDC}&#34;)
            else:
                print(f&#34;{_bcolors.WARNING}The dataset [{self.dataset}] already exists in [{directory}]!{_bcolors.ENDC}&#34;)
    def _cleanData(self, normalization, binary_label, force_clean):
        &#34;&#34;&#34;
            Load the raw dataset from self.download_dir.
            If normalization is set, it will perform appropriate normalization.
            See docstring of the getAndClean to find valid values.
            
            If the cleaned dataset already exists, then it will skip. 
            However, you can force cleaning the dataset by setting force_clean=True.
        &#34;&#34;&#34;
        directory = f&#34;{self.cleand_dir}/{self.task}&#34;
        if not os.path.exists(directory):
            os.makedirs(directory)
        is_cleaned = os.path.exists(f&#34;{directory}/{self.dataset}&#34;)
        if not is_cleaned or force_clean:
            data = load_svmlight_file(f&#34;{self.download_dir}/{self.task}/{self.dataset}&#34;)
            X, y= data[0], data[1]
            n, p = X.shape
            # check label for the binary task
            if self.task == &#39;binary&#39;:
                y1old, y2old = np.unique(y)
                if binary_label is not None:
                    if  binary_label==&#39;{-1,1}&#39;:
                        y1new, y2new = -1.0, 1.0
                    elif binary_label==&#39;{0,1}&#39;:
                        y1new, y2new = 0.0, 1.0
                    else:
                        raise ValueError(f&#34;Unrecognized binary_level: {binary_label}&#34;)
                    y[y==y1old] = y1new
                    y[y==y2old] = y2new
                    print(f&#34;Original y-label range: {{ {y1old}, {y2old} }} -&gt; New y-label range: {{ {np.unique(y)[0]}, {np.unique(y)[1]} }}&#34;)
                else:
                    raise ValueError(f&#34;{_bcolors.FAIL}You should set the desired binary_level.\n For example, binary_label=&#39;[-1,1]&#39;.{_bcolors.ENDC}&#34;)
            # check feature range
            if normalization is not None:
                print(f&#34;Perform normalization:{normalization}&#34;)
                if normalization == &#39;feat-11&#39;:
                    for i in range(p):
                        temp = X[:,i]
                        if np.max(temp) &gt; 1.0 or np.min(temp) &lt; -1.0:
                            X[:,i] /= np.max(np.abs(temp))
                            if verbose:
                                print(f&#34;  col:{i}: max:{np.max(temp):3.3e} | min:{np.min(temp):3.3e}\n&#34;\
                                       &#34;  Apply feature-wise [-1,1] scaling...&#34;)
                elif normalization == &#39;feat01&#39;:
                    for i in range(p):
                        temp = X[:,i]
                        xmax, xmin = np.max(temp), np.min(temp)
                        if xmax &gt; 1.0 or  xmin &lt; 0.0:
                            X[:,i] = (X[:,i] - xmin) / (xmax - xmin)
                            if verbose:
                                print(f&#34;  col:{i}: max:{np.max(temp):3.3e} | min:{np.min(temp):3.3e}\n&#34;\
                                       &#34;  Apply feature-wise [0,1] scaling...&#34;)
                else:
                    raise ValueError(f&#34;{_bcolors.FAIL}Unrecognized normalization: {normalization}{_bcolors.ENDC}&#34;)
            dump_svmlight_file(X, y, f&#34;{directory}/{self.dataset}&#34;)
            if os.path.exists(f&#34;{directory}/{self.dataset}&#34;):
                print(f&#34;{_bcolors.OKGREEN}Success: File saved at {directory}/{self.dataset}!{_bcolors.ENDC}&#34;)
                print(&#34;-*&#34;*30)
        else:
             print(f&#34;{_bcolors.WARNING}The cleaned dataset [{self.dataset}] already exists in [{directory}]!{_bcolors.ENDC}&#34;)
    def getAvailable(self):
            &#34;&#34;&#34;Show supported tasks and for each supported task show avaiable datasets.
            Typical usage example:
            
                   libsvm = LibsvmDataset()
                   libsvm.getAvailable()
            &#34;&#34;&#34;
            print(&#34;Current supported tasks are:&#34;)
            for k in self.task_dict.keys():
                print(f&#34; [&#39;{k}&#39;]&#34;, end=&#34;&#34;)
            print(&#34;\n=====================================&#34;)
            for k in self.task_dict.keys():
                print(f&#34;For task:[&#39;{k}&#39;], available datasets are:&#34;)
                print(&#34;----------------------------------------------------&#34;)
                dataset_lst = []
                for i,d in enumerate(self.task_dict[k][&#34;dataset&#34;]):
                    print(f&#34; &#39;{d}&#39;&#34;, end=&#34;,&#34;)
                    if (i +1) % 5 == 0:
                        print(&#34;\n&#34;)
            print(&#34;\n&#34;)
    def getAndClean(self, task=None, dataset=None, download_url=None, 
                    binary_lable=&#39;{-1,1}&#39;, normalization=&#39;feat-11&#39;,
                    force_download=False, force_clean=False, clean_verbose=True
                   ):
        &#34;&#34;&#34;Download and clean the dataset.
        Typical usage example:

              libsvm = LibsvmDataset()
              libsvm.getAndClean(task=&#34;binary&#34;, dataset=&#34;a1a&#34;, binary_lable=&#39;{-1,1}&#39;, normalization=&#39;feat-11&#39;)
              libsvm.getAndClean(task=&#34;regression&#34;, dataset=&#34;abalone&#34;, normalization=&#39;feat-11&#39;)
              libsvm.getAndClean(url=&#39;https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/avazu-app.bz2&#39;,
                                 binary_lable=&#39;{-1,1}&#39;, normalization=&#39;feat-11&#39;)              
        Args:

            task: A string specifies the task want to perform. Currently supported {&#39;binary&#39;, &#39;regression&#39;}.
            dataset: A string specifies the dataset you want to download. Use `getAvailable` method to show all
                     currently avaiable datasets for any given task.
            download_url: If the desired dataset is not provided for a given task, one can directly provide a url 
                          link to the desired data set. For example, one wants to download the avazu dataset.
                          One can visit https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html#avazu.
                          If you want to download the &#34;avazu-app.bz2&#34; instance, you can right-click its name and 
                          select &#34;copy link&#34;, then you should get a plain text as
                              &lt;https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/avazu-app.bz2&gt;
                          Then just provides it as string.
            binary_label: If you want to perform binay classification, one can set labels to {-1,1} by providing
                          &#39;{-1,1}&#39;; or set labels to {0,1} by providing &#39;{0,1}&#39;. Default to &#39;{-1,1}&#39;.
            normalization: Perform feature-wise normalization. Currently supported options:
                             &#39;feat-11&#39;: feature-wise scaling to range [-1,1]
                             &#39;feat01&#39;: feature-wise scaling to range [0,1]
                           Default to &#39;{-1,1}&#39;.
            force_download: If set to True, then download the dataset even if it already exists. Default to False.
            force_clean:    If set to True, then clean the dataset even if it already exists in clean folder. Default to False.
            clean_verbose:  If set to True, will print out which feature being normalized. Default to False.
        &#34;&#34;&#34;
        # reset
        self.download_url = None
        self.task = None
        self.dataset = None 
        self._parseInputs(task, dataset, download_url)
        if self.download_url is not None:
            self._getData(force_download)
            if self.task in [&#34;binary&#34;, &#34;regression&#34;]:
                self._cleanData(normalization, binary_lable, force_clean)
            else:
                print(f&#34;{_bcolors.WARNING}The clean rule for dataset:{self.dataset} with task:{self.task} is not defined. Hence, no cleaning is performed.{_bcolors.ENDC}&#34;)
        else:
            print(f&#34;{_bcolors.WARNING}Fail to generate download url, please check your inputs.{_bcolors.ENDC}&#34;)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="LibsvmDataset.LibsvmDataset"><code class="flex name class">
<span>class <span class="ident">LibsvmDataset</span></span>
<span>(</span><span>download_dir='./raw', cleand_dir='./clean')</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize the
LibsvmDataset class.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>download_dir</code></strong></dt>
<dd>A string specifies the place to store the downloaded raw dataset.</dd>
<dt><strong><code>cleand_dir</code></strong></dt>
<dd>A string specifies the place to store the cleaned dataset.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LibsvmDataset:
    def __init__(self, download_dir=&#34;./raw&#34;, 
                 cleand_dir=&#34;./clean&#34;):
        &#34;&#34;&#34;
        Initialize the  LibsvmDataset class.
        
        Args:
            download_dir: A string specifies the place to store the downloaded raw dataset.
            cleand_dir: A string specifies the place to store the cleaned dataset.
        &#34;&#34;&#34;
        self.download_dir = download_dir
        self.cleand_dir = cleand_dir
        for directory in [self.download_dir, self.cleand_dir]:
            if not os.path.exists(directory):
                os.makedirs(directory)
        self.url_regression = &#34;https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/regression&#34;
        self.url_binary = &#34;https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary&#34;
        self.data_binary = [
            &#39;a1a&#39;, &#39;a2a&#39;, &#39;a3a&#39;, &#39;a4a&#39;, &#39;a5a&#39;, &#39;a6a&#39;, &#39;a7a&#39;, &#39;a8a&#39;, &#39;a9a&#39;,
            &#39;a1a.t&#39;, &#39;a2a.t&#39;, &#39;a3a.t&#39;, &#39;a4a.t&#39;, &#39;a5a.t&#39;, &#39;a6a.t&#39;, &#39;a7a.t&#39;, &#39;a8a.t&#39;, &#39;a9a.t&#39;, 
            &#39;australian&#39;,
            &#39;breast-cancer&#39;,
            &#39;cod-rna&#39;, &#39;cod-rna.t&#39;, &#39;cod-rna.r&#39;, 
            &#39;colon-cancer.bz2&#39;,
            &#39;covtype.libsvm.binary.bz2&#39;,
            &#39;diabetes&#39;,
            &#39;duke.bz2&#39;,
            &#39;fourclass&#39;,
            &#39;german.numer&#39;,
            &#39;gisette_scale.bz2&#39;, &#39;gisette_scale.t.bz2&#39;,
            &#39;heart&#39;,
            &#39;ijcnn1.bz2&#39;,
            &#39;ionosphere_scale&#39;,
            &#39;leu.bz2&#39;, &#39;leu.bz2.t&#39;,
            &#39;liver-disorders&#39;, &#39;liver-disorders.t&#39;,
            &#39;madelon&#39;, &#39;madelon.t&#39;,
            &#39;mushrooms&#39;,
            &#39;news20.binary.bz2&#39;,
            &#39;phishing&#39;,
            &#39;rcv1_train.binary.bz2&#39;,&#39;rcv1_test.binary.bz2&#39;,
            &#39;real-sim.bz2&#39;, &#39;skin_nonskin&#39;,
            &#39;splice&#39;, &#39;splice.t&#39;,
            &#39;sonar_scale&#39;,
            &#39;svmguide1&#39;, &#39;svmguide1.t&#39;, &#39;svmguide3&#39;, &#39;svmguide3.t&#39;, 
            &#39;w1a&#39;, &#39;w2a&#39;, &#39;w3a&#39;, &#39;w4a&#39;, &#39;w5a&#39;, &#39;w6a&#39;, &#39;w7a&#39;, &#39;w8a&#39;,
            &#39;w1a.t&#39;, &#39;w2a.t&#39;, &#39;w3a.t&#39;, &#39;w4a.t&#39;, &#39;w5a.t&#39;, &#39;w6a.t&#39;, &#39;w7a.t&#39;, &#39;w8a.t&#39;
             #&#39;epsilon_normalized.bz2&#39;, &#39;epsilon_normalized.t.bz2&#39;
             #&#39;HIGGS.bz2&#39;,
        ]        
        self.data_regression = [
            &#39;abalone&#39;,
            &#39;bodyfat&#39;,
            &#39;cadata&#39;,
            &#39;cpusmall&#39;,
            &#39;log1p.E2006.train.bz2&#39;, &#39;log1p.E2006.test.bz2 &#39;
            &#39;E2006.train.bz2&#39;, &#39;E2006.test.bz2&#39;,
            &#39;eunite2001&#39;, &#39;eunite2001.t&#39;, &#39;eunite2001.m&#39;,
            &#39;housing&#39;,
            &#39;mg&#39;,
            &#39;mpg&#39;,
            &#39;pyrim&#39;,
            &#39;space_ga&#39;,
            &#39;triazines&#39;,
            &#39;YearPredictionMSD.bz2&#39;, &#39;YearPredictionMSD.t.bz2&#39;
        ]       
        self.task_dict = {&#34;binary&#34;:{&#34;url&#34;:self.url_binary, 
                                    &#34;dataset&#34;:self.data_binary}, 
                          &#34;regression&#34;:{&#34;url&#34;:self.url_regression, 
                                        &#34;dataset&#34;:self.data_regression}}  
        # for printing
        self.pbar = None
        
    def _show_progress(self, block_num, block_size, total_size):
        &#34;&#34;&#34;
            private function. Show the progress of urlretrieve for downloading data.
        &#34;&#34;&#34;
        if self.pbar is None:
            self.pbar = progressbar.ProgressBar(maxval=total_size)
            self.pbar.start()

        downloaded = block_num * block_size
        if downloaded &lt; total_size:
            self.pbar.update(downloaded)
        else:
            self.pbar.finish()
            self.pbar = None
    
    def _parseInputs(self, task=None, dataset=None, download_url=None):
        if task is not None and dataset is not None:
            print(&#34;You choose to use the task+dataset option.&#34;)
            try:
                work_dict = self.task_dict[task]
            except KeyError:
                print(f&#34;{_bcolors.WARNING}Warning:Your input taks is [{task}], which currently is not supported.\n&#34;\
                      f&#34;However, you can provide an url pointing to the desired dataset to download it.{_bcolors.ENDC}&#34;)
                return
            is_available = dataset in work_dict[&#34;dataset&#34;]
            if not is_available:
                print(f&#34;{_bcolors.FAIL}Error occurs!\n&#34;\
                     f&#34;  1.Either the input dataset:[{dataset}] is not intended for the task:[{task}].\n&#34;\
                     f&#34;  2.Or the input dataset:[{dataset}] is not in the built-in database.\n&#34;\
                     f&#34;If you are sure the latter case happens, you can provide an url pointing to the desired dataset.{_bcolors.ENDC}&#34;
                     )
                return
            self.download_url = work_dict[&#34;url&#34;] + &#34;/&#34; + dataset
            self.task = task
            self.dataset = dataset
        elif download_url:
            print(&#34;You choose to use the url option.&#34;)
            try:
                task, dataset = download_url.split(&#34;/&#34;)[-2], download_url.split(&#34;/&#34;)[-1]
                self.download_url = download_url
                self.task = task
                self.dataset = dataset
            except IndexError:
                self.download_url = None
                print(f&#34;{_bcolors.FAIL}The input url {download_url} is wrong.{_bcolors.ENDC}&#34;)
        else:
            raise ValueError(f&#34;{_bcolors.FAIL}Code has bugs.{_bcolors.ENDC}&#34;)
        if self.download_url:
            print(f&#34;Parsed task: [{self.task}] | Parsed dataset: [{self.dataset}]\nParsed download_url:[{self.download_url}]&#34;)
    def _getData(self, force_download):
        &#34;&#34;&#34;
            Use urllib.request::urlretrieve to download the self.dataset and save to 
            self.download_dir/self.task based on the self.download_url.
            If the dataset already exists, then it will skip. However, you can force download
            by setting force_download=True.
        &#34;&#34;&#34;
        if self.download_url is not None:
            # check whether the dataset is being downloaded or not
            directory = f&#34;{self.download_dir}/{self.task}&#34;
            if not os.path.exists(directory):
                os.makedirs(directory)
            is_downloaded = os.path.exists(f&#34;{directory}/{self.dataset}&#34;)
            if not is_downloaded or force_download:
                urlretrieve(self.download_url, f&#39;{directory}/{self.dataset}&#39;, self._show_progress)
                #print(&#34;Start downloading... It may take a while&#34;)
                #subprocess.run([&#39;wget&#39;, &#39;-i&#39;, self.download_url, &#39;-P&#39;, self.download_dir, 
                #                &#39;-O&#39;, f&#39;{self.download_dir}/{self.dataset}&#39;])
                if os.path.exists(f&#34;{directory}/{self.dataset}&#34;):
                    print(f&#34;{_bcolors.OKGREEN}dataset [{self.dataset}] is downloaded at [{directory}].{_bcolors.ENDC}&#34;)
            else:
                print(f&#34;{_bcolors.WARNING}The dataset [{self.dataset}] already exists in [{directory}]!{_bcolors.ENDC}&#34;)
    def _cleanData(self, normalization, binary_label, force_clean):
        &#34;&#34;&#34;
            Load the raw dataset from self.download_dir.
            If normalization is set, it will perform appropriate normalization.
            See docstring of the getAndClean to find valid values.
            
            If the cleaned dataset already exists, then it will skip. 
            However, you can force cleaning the dataset by setting force_clean=True.
        &#34;&#34;&#34;
        directory = f&#34;{self.cleand_dir}/{self.task}&#34;
        if not os.path.exists(directory):
            os.makedirs(directory)
        is_cleaned = os.path.exists(f&#34;{directory}/{self.dataset}&#34;)
        if not is_cleaned or force_clean:
            data = load_svmlight_file(f&#34;{self.download_dir}/{self.task}/{self.dataset}&#34;)
            X, y= data[0], data[1]
            n, p = X.shape
            # check label for the binary task
            if self.task == &#39;binary&#39;:
                y1old, y2old = np.unique(y)
                if binary_label is not None:
                    if  binary_label==&#39;{-1,1}&#39;:
                        y1new, y2new = -1.0, 1.0
                    elif binary_label==&#39;{0,1}&#39;:
                        y1new, y2new = 0.0, 1.0
                    else:
                        raise ValueError(f&#34;Unrecognized binary_level: {binary_label}&#34;)
                    y[y==y1old] = y1new
                    y[y==y2old] = y2new
                    print(f&#34;Original y-label range: {{ {y1old}, {y2old} }} -&gt; New y-label range: {{ {np.unique(y)[0]}, {np.unique(y)[1]} }}&#34;)
                else:
                    raise ValueError(f&#34;{_bcolors.FAIL}You should set the desired binary_level.\n For example, binary_label=&#39;[-1,1]&#39;.{_bcolors.ENDC}&#34;)
            # check feature range
            if normalization is not None:
                print(f&#34;Perform normalization:{normalization}&#34;)
                if normalization == &#39;feat-11&#39;:
                    for i in range(p):
                        temp = X[:,i]
                        if np.max(temp) &gt; 1.0 or np.min(temp) &lt; -1.0:
                            X[:,i] /= np.max(np.abs(temp))
                            if verbose:
                                print(f&#34;  col:{i}: max:{np.max(temp):3.3e} | min:{np.min(temp):3.3e}\n&#34;\
                                       &#34;  Apply feature-wise [-1,1] scaling...&#34;)
                elif normalization == &#39;feat01&#39;:
                    for i in range(p):
                        temp = X[:,i]
                        xmax, xmin = np.max(temp), np.min(temp)
                        if xmax &gt; 1.0 or  xmin &lt; 0.0:
                            X[:,i] = (X[:,i] - xmin) / (xmax - xmin)
                            if verbose:
                                print(f&#34;  col:{i}: max:{np.max(temp):3.3e} | min:{np.min(temp):3.3e}\n&#34;\
                                       &#34;  Apply feature-wise [0,1] scaling...&#34;)
                else:
                    raise ValueError(f&#34;{_bcolors.FAIL}Unrecognized normalization: {normalization}{_bcolors.ENDC}&#34;)
            dump_svmlight_file(X, y, f&#34;{directory}/{self.dataset}&#34;)
            if os.path.exists(f&#34;{directory}/{self.dataset}&#34;):
                print(f&#34;{_bcolors.OKGREEN}Success: File saved at {directory}/{self.dataset}!{_bcolors.ENDC}&#34;)
                print(&#34;-*&#34;*30)
        else:
             print(f&#34;{_bcolors.WARNING}The cleaned dataset [{self.dataset}] already exists in [{directory}]!{_bcolors.ENDC}&#34;)
    def getAvailable(self):
            &#34;&#34;&#34;Show supported tasks and for each supported task show avaiable datasets.
            Typical usage example:
            
                   libsvm = LibsvmDataset()
                   libsvm.getAvailable()
            &#34;&#34;&#34;
            print(&#34;Current supported tasks are:&#34;)
            for k in self.task_dict.keys():
                print(f&#34; [&#39;{k}&#39;]&#34;, end=&#34;&#34;)
            print(&#34;\n=====================================&#34;)
            for k in self.task_dict.keys():
                print(f&#34;For task:[&#39;{k}&#39;], available datasets are:&#34;)
                print(&#34;----------------------------------------------------&#34;)
                dataset_lst = []
                for i,d in enumerate(self.task_dict[k][&#34;dataset&#34;]):
                    print(f&#34; &#39;{d}&#39;&#34;, end=&#34;,&#34;)
                    if (i +1) % 5 == 0:
                        print(&#34;\n&#34;)
            print(&#34;\n&#34;)
    def getAndClean(self, task=None, dataset=None, download_url=None, 
                    binary_lable=&#39;{-1,1}&#39;, normalization=&#39;feat-11&#39;,
                    force_download=False, force_clean=False, clean_verbose=True
                   ):
        &#34;&#34;&#34;Download and clean the dataset.
        Typical usage example:

              libsvm = LibsvmDataset()
              libsvm.getAndClean(task=&#34;binary&#34;, dataset=&#34;a1a&#34;, binary_lable=&#39;{-1,1}&#39;, normalization=&#39;feat-11&#39;)
              libsvm.getAndClean(task=&#34;regression&#34;, dataset=&#34;abalone&#34;, normalization=&#39;feat-11&#39;)
              libsvm.getAndClean(url=&#39;https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/avazu-app.bz2&#39;,
                                 binary_lable=&#39;{-1,1}&#39;, normalization=&#39;feat-11&#39;)              
        Args:

            task: A string specifies the task want to perform. Currently supported {&#39;binary&#39;, &#39;regression&#39;}.
            dataset: A string specifies the dataset you want to download. Use `getAvailable` method to show all
                     currently avaiable datasets for any given task.
            download_url: If the desired dataset is not provided for a given task, one can directly provide a url 
                          link to the desired data set. For example, one wants to download the avazu dataset.
                          One can visit https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html#avazu.
                          If you want to download the &#34;avazu-app.bz2&#34; instance, you can right-click its name and 
                          select &#34;copy link&#34;, then you should get a plain text as
                              &lt;https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/avazu-app.bz2&gt;
                          Then just provides it as string.
            binary_label: If you want to perform binay classification, one can set labels to {-1,1} by providing
                          &#39;{-1,1}&#39;; or set labels to {0,1} by providing &#39;{0,1}&#39;. Default to &#39;{-1,1}&#39;.
            normalization: Perform feature-wise normalization. Currently supported options:
                             &#39;feat-11&#39;: feature-wise scaling to range [-1,1]
                             &#39;feat01&#39;: feature-wise scaling to range [0,1]
                           Default to &#39;{-1,1}&#39;.
            force_download: If set to True, then download the dataset even if it already exists. Default to False.
            force_clean:    If set to True, then clean the dataset even if it already exists in clean folder. Default to False.
            clean_verbose:  If set to True, will print out which feature being normalized. Default to False.
        &#34;&#34;&#34;
        # reset
        self.download_url = None
        self.task = None
        self.dataset = None 
        self._parseInputs(task, dataset, download_url)
        if self.download_url is not None:
            self._getData(force_download)
            if self.task in [&#34;binary&#34;, &#34;regression&#34;]:
                self._cleanData(normalization, binary_lable, force_clean)
            else:
                print(f&#34;{_bcolors.WARNING}The clean rule for dataset:{self.dataset} with task:{self.task} is not defined. Hence, no cleaning is performed.{_bcolors.ENDC}&#34;)
        else:
            print(f&#34;{_bcolors.WARNING}Fail to generate download url, please check your inputs.{_bcolors.ENDC}&#34;)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="LibsvmDataset.LibsvmDataset.getAndClean"><code class="name flex">
<span>def <span class="ident">getAndClean</span></span>(<span>self, task=None, dataset=None, download_url=None, binary_lable='{-1,1}', normalization='feat-11', force_download=False, force_clean=False, clean_verbose=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Download and clean the dataset.
Typical usage example:</p>
<pre><code>  libsvm = LibsvmDataset()
  libsvm.getAndClean(task="binary", dataset="a1a", binary_lable='{-1,1}', normalization='feat-11')
  libsvm.getAndClean(task="regression", dataset="abalone", normalization='feat-11')
  libsvm.getAndClean(url='https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/avazu-app.bz2',
                     binary_lable='{-1,1}', normalization='feat-11')
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>task</code></strong></dt>
<dd>A string specifies the task want to perform. Currently supported {'binary', 'regression'}.</dd>
<dt><strong><code>dataset</code></strong></dt>
<dd>A string specifies the dataset you want to download. Use <code>getAvailable</code> method to show all
currently avaiable datasets for any given task.</dd>
<dt><strong><code>download_url</code></strong></dt>
<dd>If the desired dataset is not provided for a given task, one can directly provide a url
link to the desired data set. For example, one wants to download the avazu dataset.
One can visit <a href="https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html#avazu.">https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html#avazu.</a>
If you want to download the "avazu-app.bz2" instance, you can right-click its name and
select "copy link", then you should get a plain text as
<a href="https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/avazu-app.bz2">https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/avazu-app.bz2</a>
Then just provides it as string.</dd>
<dt><strong><code>binary_label</code></strong></dt>
<dd>If you want to perform binay classification, one can set labels to {-1,1} by providing
'{-1,1}'; or set labels to {0,1} by providing '{0,1}'. Default to '{-1,1}'.</dd>
<dt><strong><code>normalization</code></strong></dt>
<dd>Perform feature-wise normalization. Currently supported options:
'feat-11': feature-wise scaling to range [-1,1]
'feat01': feature-wise scaling to range [0,1]
Default to '{-1,1}'.</dd>
<dt><strong><code>force_download</code></strong></dt>
<dd>If set to True, then download the dataset even if it already exists. Default to False.</dd>
<dt><strong><code>force_clean</code></strong></dt>
<dd>If set to True, then clean the dataset even if it already exists in clean folder. Default to False.</dd>
<dt><strong><code>clean_verbose</code></strong></dt>
<dd>If set to True, will print out which feature being normalized. Default to False.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def getAndClean(self, task=None, dataset=None, download_url=None, 
                binary_lable=&#39;{-1,1}&#39;, normalization=&#39;feat-11&#39;,
                force_download=False, force_clean=False, clean_verbose=True
               ):
    &#34;&#34;&#34;Download and clean the dataset.
    Typical usage example:

          libsvm = LibsvmDataset()
          libsvm.getAndClean(task=&#34;binary&#34;, dataset=&#34;a1a&#34;, binary_lable=&#39;{-1,1}&#39;, normalization=&#39;feat-11&#39;)
          libsvm.getAndClean(task=&#34;regression&#34;, dataset=&#34;abalone&#34;, normalization=&#39;feat-11&#39;)
          libsvm.getAndClean(url=&#39;https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/avazu-app.bz2&#39;,
                             binary_lable=&#39;{-1,1}&#39;, normalization=&#39;feat-11&#39;)              
    Args:

        task: A string specifies the task want to perform. Currently supported {&#39;binary&#39;, &#39;regression&#39;}.
        dataset: A string specifies the dataset you want to download. Use `getAvailable` method to show all
                 currently avaiable datasets for any given task.
        download_url: If the desired dataset is not provided for a given task, one can directly provide a url 
                      link to the desired data set. For example, one wants to download the avazu dataset.
                      One can visit https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html#avazu.
                      If you want to download the &#34;avazu-app.bz2&#34; instance, you can right-click its name and 
                      select &#34;copy link&#34;, then you should get a plain text as
                          &lt;https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/avazu-app.bz2&gt;
                      Then just provides it as string.
        binary_label: If you want to perform binay classification, one can set labels to {-1,1} by providing
                      &#39;{-1,1}&#39;; or set labels to {0,1} by providing &#39;{0,1}&#39;. Default to &#39;{-1,1}&#39;.
        normalization: Perform feature-wise normalization. Currently supported options:
                         &#39;feat-11&#39;: feature-wise scaling to range [-1,1]
                         &#39;feat01&#39;: feature-wise scaling to range [0,1]
                       Default to &#39;{-1,1}&#39;.
        force_download: If set to True, then download the dataset even if it already exists. Default to False.
        force_clean:    If set to True, then clean the dataset even if it already exists in clean folder. Default to False.
        clean_verbose:  If set to True, will print out which feature being normalized. Default to False.
    &#34;&#34;&#34;
    # reset
    self.download_url = None
    self.task = None
    self.dataset = None 
    self._parseInputs(task, dataset, download_url)
    if self.download_url is not None:
        self._getData(force_download)
        if self.task in [&#34;binary&#34;, &#34;regression&#34;]:
            self._cleanData(normalization, binary_lable, force_clean)
        else:
            print(f&#34;{_bcolors.WARNING}The clean rule for dataset:{self.dataset} with task:{self.task} is not defined. Hence, no cleaning is performed.{_bcolors.ENDC}&#34;)
    else:
        print(f&#34;{_bcolors.WARNING}Fail to generate download url, please check your inputs.{_bcolors.ENDC}&#34;)</code></pre>
</details>
</dd>
<dt id="LibsvmDataset.LibsvmDataset.getAvailable"><code class="name flex">
<span>def <span class="ident">getAvailable</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Show supported tasks and for each supported task show avaiable datasets.
Typical usage example:</p>
<pre><code>   libsvm = LibsvmDataset()
   libsvm.getAvailable()
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def getAvailable(self):
        &#34;&#34;&#34;Show supported tasks and for each supported task show avaiable datasets.
        Typical usage example:
        
               libsvm = LibsvmDataset()
               libsvm.getAvailable()
        &#34;&#34;&#34;
        print(&#34;Current supported tasks are:&#34;)
        for k in self.task_dict.keys():
            print(f&#34; [&#39;{k}&#39;]&#34;, end=&#34;&#34;)
        print(&#34;\n=====================================&#34;)
        for k in self.task_dict.keys():
            print(f&#34;For task:[&#39;{k}&#39;], available datasets are:&#34;)
            print(&#34;----------------------------------------------------&#34;)
            dataset_lst = []
            for i,d in enumerate(self.task_dict[k][&#34;dataset&#34;]):
                print(f&#34; &#39;{d}&#39;&#34;, end=&#34;,&#34;)
                if (i +1) % 5 == 0:
                    print(&#34;\n&#34;)
        print(&#34;\n&#34;)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="LibsvmDataset.LibsvmDataset" href="#LibsvmDataset.LibsvmDataset">LibsvmDataset</a></code></h4>
<ul class="">
<li><code><a title="LibsvmDataset.LibsvmDataset.getAndClean" href="#LibsvmDataset.LibsvmDataset.getAndClean">getAndClean</a></code></li>
<li><code><a title="LibsvmDataset.LibsvmDataset.getAvailable" href="#LibsvmDataset.LibsvmDataset.getAvailable">getAvailable</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>